#### general settings
name: train_small_model
scale: 4
type : mse


#### datasets
datasets:
  train:
    name: train
    mode: LQGT
    noise_rate_random: false
    noise_needed: false
    noise_data: ["../../storage/data/df2k/color_noise/","../../storage/data/df2k/jepg_noise/Corrupted_noise/" ]
    dataroot_GT: ../../storage/data/df2k/HR
    dataroot_LQ: ../../storage/data/df2k/LRx4
    use_shuffle: false
    n_workers: 4  # per GPU
    batch_size: 64
    GT_size: 128
    color: RGB
  val:
    name: validation
    mode: LQGT
    noise_rate_random: true
    noise_needed: false
    noise_data: ../../storage/data/df2k/Corrupted_noise/
    dataroot_GT: ../../storage/data/df2k/HR
    dataroot_LQ: ../../storage/data/df2k/LRx4
    use_shuffle: false
    n_workers: 4  # per GPU
    batch_size: 1
    GT_size: 512 
    color: RGB
#### network structures
structure:
  network_G:
    which_model_G: imdn 
    in_nc: 3
    out_nc: 3
    nf: 64 
    num_modules: 8 

#### path
pretraining_settings:
  network_G: 
    want_load: true
    pretrained_model_path:  "../checkpoints/verysmall_best_charbonnier_x4_imdn_rtc_20_6.pt"
    key: "generator_state_dict"
    strict_load:  true
  
#epoch settings
epoch_settings:
  total_epochs: 100
  resume_state_epoch: ~
  resume_state_batch: ~

#### training settings: learning rate scheme, loss
train_settings:
  lr_G: !!float 2.5e-05
  lr_step_decay: 100
  weight_decay_G: !!float 0
  beta1_G: 0.9
  beta2_G: 0.999
  pixel_criterion: WassFeatureLoss #l1 | l2  #CharbonnierLoss
  top_score:  !!int -1
  
  sample_interval: ~
  save_checkpoint_folder_path: "../checkpoints"
  save_checkpoint_file_name: "gantest.pt"
  save_bestmodel_file_name: "gantest.pt"
